<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>3.5 Boosting en R | Aprendizaje Estadístico</title>
  <meta name="description" content="Apuntes de la asignatura de Aprendizaje Estadístico del Máster en Técnicas Estadísticas." />
  <meta name="generator" content="bookdown 0.18 and GitBook 2.6.7" />

  <meta property="og:title" content="3.5 Boosting en R | Aprendizaje Estadístico" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="Apuntes de la asignatura de Aprendizaje Estadístico del Máster en Técnicas Estadísticas." />
  <meta name="github-repo" content="rubenfcasal/aprendizaje_estadistico" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="3.5 Boosting en R | Aprendizaje Estadístico" />
  
  <meta name="twitter:description" content="Apuntes de la asignatura de Aprendizaje Estadístico del Máster en Técnicas Estadísticas." />
  

<meta name="author" content="Rubén Fernández Casal (ruben.fcasal@udc.es), Julián Costa (julian.costa@udc.es)" />


<meta name="date" content="2020-11-03" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="boosting.html"/>
<link rel="next" href="referencias.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<script src="libs/htmlwidgets-1.5.1/htmlwidgets.js"></script>
<link href="libs/datatables-css-0.0.0/datatables-crosstalk.css" rel="stylesheet" />
<script src="libs/datatables-binding-0.13/datatables.js"></script>
<link href="libs/dt-core-1.10.20/css/jquery.dataTables.min.css" rel="stylesheet" />
<link href="libs/dt-core-1.10.20/css/jquery.dataTables.extra.css" rel="stylesheet" />
<script src="libs/dt-core-1.10.20/js/jquery.dataTables.min.js"></script>
<link href="libs/crosstalk-1.1.0.1/css/crosstalk.css" rel="stylesheet" />
<script src="libs/crosstalk-1.1.0.1/js/crosstalk.min.js"></script>


<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Aprendizaje Estadístico</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Prólogo</a></li>
<li class="chapter" data-level="1" data-path="intro-AE.html"><a href="intro-AE.html"><i class="fa fa-check"></i><b>1</b> Introducción al Aprendizaje Estadístico</a><ul>
<li class="chapter" data-level="1.1" data-path="aprendizaje-estadístico-vs-aprendizaje-automático.html"><a href="aprendizaje-estadístico-vs-aprendizaje-automático.html"><i class="fa fa-check"></i><b>1.1</b> Aprendizaje Estadístico vs. Aprendizaje Automático</a><ul>
<li class="chapter" data-level="1.1.1" data-path="aprendizaje-estadístico-vs-aprendizaje-automático.html"><a href="aprendizaje-estadístico-vs-aprendizaje-automático.html#machine-learning-vs.data-mining"><i class="fa fa-check"></i><b>1.1.1</b> Machine Learning vs. Data Mining</a></li>
<li class="chapter" data-level="1.1.2" data-path="aprendizaje-estadístico-vs-aprendizaje-automático.html"><a href="aprendizaje-estadístico-vs-aprendizaje-automático.html#las-dos-culturas-breiman-2001"><i class="fa fa-check"></i><b>1.1.2</b> Las dos culturas (Breiman, 2001)</a></li>
<li class="chapter" data-level="1.1.3" data-path="aprendizaje-estadístico-vs-aprendizaje-automático.html"><a href="aprendizaje-estadístico-vs-aprendizaje-automático.html#machine-learning-vs.estadística-dunson-2018"><i class="fa fa-check"></i><b>1.1.3</b> Machine Learning vs. Estadística (Dunson, 2018)</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="métodos-de-aprendizaje-estadístico.html"><a href="métodos-de-aprendizaje-estadístico.html"><i class="fa fa-check"></i><b>1.2</b> Métodos de Aprendizaje Estadístico</a><ul>
<li class="chapter" data-level="1.2.1" data-path="métodos-de-aprendizaje-estadístico.html"><a href="métodos-de-aprendizaje-estadístico.html#notacion"><i class="fa fa-check"></i><b>1.2.1</b> Notación y terminología</a></li>
<li class="chapter" data-level="1.2.2" data-path="métodos-de-aprendizaje-estadístico.html"><a href="métodos-de-aprendizaje-estadístico.html#metodos-pkgs"><i class="fa fa-check"></i><b>1.2.2</b> Métodos (de aprendizaje supervisado) y paquetes de R</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="const-eval.html"><a href="const-eval.html"><i class="fa fa-check"></i><b>1.3</b> Construcción y evaluación de los modelos</a><ul>
<li class="chapter" data-level="1.3.1" data-path="const-eval.html"><a href="const-eval.html#bias-variance"><i class="fa fa-check"></i><b>1.3.1</b> Equilibrio entre sesgo y varianza: infraajuste y sobreajuste</a></li>
<li class="chapter" data-level="1.3.2" data-path="const-eval.html"><a href="const-eval.html#entrenamiento-test"><i class="fa fa-check"></i><b>1.3.2</b> Datos de entrenamiento y datos de test</a></li>
<li class="chapter" data-level="1.3.3" data-path="const-eval.html"><a href="const-eval.html#cv"><i class="fa fa-check"></i><b>1.3.3</b> Validación cruzada</a></li>
<li class="chapter" data-level="1.3.4" data-path="const-eval.html"><a href="const-eval.html#eval-reg"><i class="fa fa-check"></i><b>1.3.4</b> Evaluación de un método de regresión</a></li>
<li class="chapter" data-level="1.3.5" data-path="const-eval.html"><a href="const-eval.html#eval-class"><i class="fa fa-check"></i><b>1.3.5</b> Evaluación de un método de clasificación</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="la-maldición-de-la-dimensionalidad.html"><a href="la-maldición-de-la-dimensionalidad.html"><i class="fa fa-check"></i><b>1.4</b> La maldición de la dimensionalidad</a></li>
<li class="chapter" data-level="1.5" data-path="analisis-modelos.html"><a href="analisis-modelos.html"><i class="fa fa-check"></i><b>1.5</b> Análisis e interpretación de los modelos</a></li>
<li class="chapter" data-level="1.6" data-path="caret.html"><a href="caret.html"><i class="fa fa-check"></i><b>1.6</b> Introducción al paquete <code>caret</code></a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="trees.html"><a href="trees.html"><i class="fa fa-check"></i><b>2</b> Árboles de decisión</a><ul>
<li class="chapter" data-level="2.1" data-path="árboles-de-regresión-cart.html"><a href="árboles-de-regresión-cart.html"><i class="fa fa-check"></i><b>2.1</b> Árboles de regresión CART</a></li>
<li class="chapter" data-level="2.2" data-path="árboles-de-clasificación-cart.html"><a href="árboles-de-clasificación-cart.html"><i class="fa fa-check"></i><b>2.2</b> Árboles de clasificación CART</a></li>
<li class="chapter" data-level="2.3" data-path="cart-con-el-paquete-rpart.html"><a href="cart-con-el-paquete-rpart.html"><i class="fa fa-check"></i><b>2.3</b> CART con el paquete <code>rpart</code></a><ul>
<li class="chapter" data-level="2.3.1" data-path="cart-con-el-paquete-rpart.html"><a href="cart-con-el-paquete-rpart.html#ejemplo-regresión"><i class="fa fa-check"></i><b>2.3.1</b> Ejemplo: regresión</a></li>
<li class="chapter" data-level="2.3.2" data-path="cart-con-el-paquete-rpart.html"><a href="cart-con-el-paquete-rpart.html#class-rpart"><i class="fa fa-check"></i><b>2.3.2</b> Ejemplo: modelo de clasificación</a></li>
<li class="chapter" data-level="2.3.3" data-path="cart-con-el-paquete-rpart.html"><a href="cart-con-el-paquete-rpart.html#interfaz-de-caret"><i class="fa fa-check"></i><b>2.3.3</b> Interfaz de <code>caret</code></a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="alternativas-a-los-árboles-cart.html"><a href="alternativas-a-los-árboles-cart.html"><i class="fa fa-check"></i><b>2.4</b> Alternativas a los árboles CART</a><ul>
<li class="chapter" data-level="2.4.1" data-path="alternativas-a-los-árboles-cart.html"><a href="alternativas-a-los-árboles-cart.html#ejemplo"><i class="fa fa-check"></i><b>2.4.1</b> Ejemplo</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="bagging-boosting.html"><a href="bagging-boosting.html"><i class="fa fa-check"></i><b>3</b> Bagging y Boosting</a><ul>
<li class="chapter" data-level="3.1" data-path="bagging.html"><a href="bagging.html"><i class="fa fa-check"></i><b>3.1</b> Bagging</a></li>
<li class="chapter" data-level="3.2" data-path="bosques-aleatorios.html"><a href="bosques-aleatorios.html"><i class="fa fa-check"></i><b>3.2</b> Bosques aleatorios</a></li>
<li class="chapter" data-level="3.3" data-path="bagging-rf-r.html"><a href="bagging-rf-r.html"><i class="fa fa-check"></i><b>3.3</b> Bagging y bosques aleatorios en R</a><ul>
<li class="chapter" data-level="3.3.1" data-path="bagging-rf-r.html"><a href="bagging-rf-r.html#ejemplo-clasificación-con-bagging"><i class="fa fa-check"></i><b>3.3.1</b> Ejemplo: Clasificación con bagging</a></li>
<li class="chapter" data-level="3.3.2" data-path="bagging-rf-r.html"><a href="bagging-rf-r.html#ejemplo-clasificación-con-bosques-aleatorios"><i class="fa fa-check"></i><b>3.3.2</b> Ejemplo: Clasificación con bosques aleatorios</a></li>
<li class="chapter" data-level="3.3.3" data-path="bagging-rf-r.html"><a href="bagging-rf-r.html#ejemplo-bosques-aleatorios-con-caret"><i class="fa fa-check"></i><b>3.3.3</b> Ejemplo: bosques aleatorios con <code>caret</code></a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="boosting.html"><a href="boosting.html"><i class="fa fa-check"></i><b>3.4</b> Boosting</a></li>
<li class="chapter" data-level="3.5" data-path="boosting-en-r.html"><a href="boosting-en-r.html"><i class="fa fa-check"></i><b>3.5</b> Boosting en R</a><ul>
<li class="chapter" data-level="3.5.1" data-path="boosting-en-r.html"><a href="boosting-en-r.html#ejemplo-clasificación-con-el-paquete-ada"><i class="fa fa-check"></i><b>3.5.1</b> Ejemplo: clasificación con el paquete <code>ada</code></a></li>
<li class="chapter" data-level="3.5.2" data-path="boosting-en-r.html"><a href="boosting-en-r.html#ejemplo-regresión-con-el-paquete-gbm"><i class="fa fa-check"></i><b>3.5.2</b> Ejemplo: regresión con el paquete <code>gbm</code></a></li>
<li class="chapter" data-level="3.5.3" data-path="boosting-en-r.html"><a href="boosting-en-r.html#ejemplo-xgboost-con-el-paquete-caret"><i class="fa fa-check"></i><b>3.5.3</b> Ejemplo: XGBoost con el paquete <code>caret</code></a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="referencias.html"><a href="referencias.html"><i class="fa fa-check"></i>Referencias</a><ul>
<li class="chapter" data-level="" data-path="bibliografía-básica.html"><a href="bibliografía-básica.html"><i class="fa fa-check"></i>Bibliografía básica</a></li>
<li class="chapter" data-level="" data-path="bibliografía-complementaria.html"><a href="bibliografía-complementaria.html"><i class="fa fa-check"></i>Bibliografía complementaria</a><ul>
<li class="chapter" data-level="" data-path="bibliografía-complementaria.html"><a href="bibliografía-complementaria.html#libros"><i class="fa fa-check"></i>Libros</a></li>
<li class="chapter" data-level="" data-path="bibliografía-complementaria.html"><a href="bibliografía-complementaria.html#artículos"><i class="fa fa-check"></i>Artículos</a></li>
</ul></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Aprendizaje Estadístico</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="boosting-en-r" class="section level2">
<h2><span class="header-section-number">3.5</span> Boosting en R</h2>
<!-- 
Búsquedas en caret: boost 
Ver [CRAN Task View: Machine Learning & Statistical Learning](https://cran.r-project.org/web/views/MachineLearning.html))
-->
<p>Estos métodos son también de los más populares en AE y están implementados en numerosos paquetes de R: <a href="https://CRAN.R-project.org/package=ada"><code>ada</code></a>, <a href="https://CRAN.R-project.org/package=adabag"><code>adabag</code></a>, <a href="https://CRAN.R-project.org/package=mboost"><code>mboost</code></a>, <a href="https://CRAN.R-project.org/package=gbm"><code>gbm</code></a>, <a href="https://github.com/dmlc/xgboost/tree/master/R-package"><code>xgboost</code></a>…</p>
<div id="ejemplo-clasificación-con-el-paquete-ada" class="section level3">
<h3><span class="header-section-number">3.5.1</span> Ejemplo: clasificación con el paquete <code>ada</code></h3>
<p>La función <code>ada()</code> del paquete <a href="https://CRAN.R-project.org/package=ada"><code>ada</code></a> (<a href="https://www.jstatsoft.org/article/view/v017i02">Culp <em>et al</em>., 2006</a>) implementa diversos métodos boosting (incluyendo el algoritmo original AdaBoost). Emplea <code>rpart</code> para la construcción de los árboles, aunque solo admite respuestas dicotómicas y dos funciones de pérdida (exponencial y logística). Además, un posible problema al emplear esta función es que ordena alfabéticamente los niveles del factor, lo que puede llevar a una mala interpretación de los resultados.</p>
<p>Los principales parámetros son los siguientes:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">ada</span>(formula, data, <span class="dt">loss =</span> <span class="kw">c</span>(<span class="st">&quot;exponential&quot;</span>, <span class="st">&quot;logistic&quot;</span>),
    <span class="dt">type =</span> <span class="kw">c</span>(<span class="st">&quot;discrete&quot;</span>, <span class="st">&quot;real&quot;</span>, <span class="st">&quot;gentle&quot;</span>), <span class="dt">iter =</span> <span class="dv">50</span>, 
    <span class="dt">nu =</span> <span class="fl">0.1</span>, <span class="dt">bag.frac =</span> <span class="fl">0.5</span>, ...)</code></pre></div>
<ul>
<li><p><code>formula</code> y <code>data</code> (opcional): permiten especificar la respuesta y las variables predictoras de la forma habitual (típicamente <code>respuesta ~ .</code>; también admite matrices <code>x</code> e <code>y</code> en lugar de fórmulas).</p></li>
<li><p><code>loss</code>: función de pérdida; por defecto <code>&quot;exponential&quot;</code> (algoritmo AdaBoost).</p></li>
<li><p><code>type</code>: algoritmo boosting; por defecto <code>&quot;discrete&quot;</code> que implementa el algoritmo AdaBoost original que predice la variable respuesta. Otras alternativas son <code>&quot;real&quot;</code>, que implementa el algoritmo <em>Real AdaBoost</em> (<a href="https://projecteuclid.org/euclid.aos/1016218223">Friedman <em>et al</em>., 2000</a>) que permite estimar las probabilidades, y <code>&quot;gentle&quot;</code> , versión modificada del anterior que emplea un método Newton de optimización por pasos (en lugar de optimización exacta).</p></li>
<li><p><code>iter</code>: número de iteraciones boosting; por defecto 50.</p></li>
<li><p><code>nu</code>: parámetro de regularización <span class="math inline">\(\lambda\)</span>; por defecto 0.1 (disminuyendo este parámetro es de esperar que se obtenga una mejora en la precisión de las predicciones pero requería aumentar <code>iter</code> aumentando notablemente el tiempo de computación y los requerimientos de memoria).</p></li>
<li><p><code>bag.frac</code>: proporción de observaciones seleccionadas al azar para crecer cada árbol; por defecto 0.5.</p></li>
<li><p><code>...</code>: argumentos adicionales para <code>rpart.control</code>; por defecto <code>rpart.control(maxdepth = 1, cp = -1, minsplit = 0, xval = 0)</code>.</p></li>
</ul>
<p>Como ejemplo consideraremos el conjunto de datos de calidad de vino empleado en las secciones <a href="cart-con-el-paquete-rpart.html#class-rpart">2.3.2</a> y <a href="bagging-rf-r.html#bagging-rf-r">3.3</a>, pero para evitar problemas reordenamos alfabéticamente los niveles de la respuesta.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">load</span>(<span class="st">&quot;data/winetaste.RData&quot;</span>)
<span class="co"># Reordenar alfabéticamente los niveles de winetaste$taste</span>
<span class="co"># winetaste$taste &lt;- factor(winetaste$taste, sort(levels(winetaste$taste)))</span>
winetaste<span class="op">$</span>taste &lt;-<span class="st"> </span><span class="kw">factor</span>(<span class="kw">as.character</span>(winetaste<span class="op">$</span>taste))
<span class="co"># Partición de los datos</span>
<span class="kw">set.seed</span>(<span class="dv">1</span>)
df &lt;-<span class="st"> </span>winetaste
nobs &lt;-<span class="st"> </span><span class="kw">nrow</span>(df)
itrain &lt;-<span class="st"> </span><span class="kw">sample</span>(nobs, <span class="fl">0.8</span> <span class="op">*</span><span class="st"> </span>nobs)
train &lt;-<span class="st"> </span>df[itrain, ]
test &lt;-<span class="st"> </span>df[<span class="op">-</span>itrain, ]</code></pre></div>
<p>Por ejemplo, el siguiente código llama a la función <code>ada()</code> con la opción para estimar probabilidades (<code>type = &quot;real&quot;</code>, Real AdaBoost), considerando interacciones (de orden 2) entre los predictores (<code>maxdepth = 2</code>), disminuyendo ligeramente el valor del parámetro de aprendizaje y aumentando el número de iteraciones:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(ada)
ada.boost &lt;-<span class="st"> </span><span class="kw">ada</span>(taste <span class="op">~</span><span class="st"> </span>., <span class="dt">data =</span> train, <span class="dt">type =</span> <span class="st">&quot;real&quot;</span>,
             <span class="dt">control =</span> <span class="kw">rpart.control</span>(<span class="dt">maxdepth =</span> <span class="dv">2</span>, <span class="dt">cp =</span> <span class="dv">0</span>, <span class="dt">minsplit =</span> <span class="dv">10</span>, <span class="dt">xval =</span> <span class="dv">0</span>),
             <span class="dt">iter =</span> <span class="dv">100</span>, <span class="dt">nu =</span> <span class="fl">0.05</span>)
ada.boost</code></pre></div>
<pre><code>## Call:
## ada(taste ~ ., data = train, type = &quot;real&quot;, control = rpart.control(maxdepth = 2, 
##     cp = 0, minsplit = 10, xval = 0), iter = 100, nu = 0.05)
## 
## Loss: exponential Method: real   Iteration: 100 
## 
## Final Confusion Matrix for Data:
##           Final Prediction
## True value bad good
##       bad  162  176
##       good  46  616
## 
## Train Error: 0.222 
## 
## Out-Of-Bag Error:  0.233  iteration= 99 
## 
## Additional Estimates of number of iterations:
## 
## train.err1 train.kap1 
##         93         93</code></pre>
<p>Con el método <code>plot()</code> podemos representar la evolución del error de clasificación al aumentar el número de iteraciones:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">plot</span>(ada.boost)</code></pre></div>
<p><img src="03-bagging_boosting_files/figure-html/unnamed-chunk-20-1.png" width="80%" style="display: block; margin: auto;" /></p>
<!-- 
Con la función `varplot()` podemos representar la importancia de las variables (y almacenarla empleando `type = "scores"`): 


```r
res <- varplot(ada.boost, type = "scores")
```

<img src="03-bagging_boosting_files/figure-html/unnamed-chunk-21-1.png" width="80%" style="display: block; margin: auto;" />

```r
res
```

```
##              density total.sulfur.dioxide            chlorides 
##           0.07518301           0.06886369           0.06586297 
##                   pH       residual.sugar        fixed.acidity 
##           0.06048902           0.05672229           0.05605724 
##          citric.acid     volatile.acidity            sulphates 
##           0.05551034           0.05074925           0.04915199 
##  free.sulfur.dioxide              alcohol 
##           0.04799147           0.04522676
```
-->
<p>Podemos evaluar la precisión en la muestra de test empleando el procedimiento habitual:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">pred &lt;-<span class="st"> </span><span class="kw">predict</span>(ada.boost, <span class="dt">newdata =</span> test)
caret<span class="op">::</span><span class="kw">confusionMatrix</span>(pred, test<span class="op">$</span>taste, <span class="dt">positive =</span> <span class="st">&quot;good&quot;</span>)</code></pre></div>
<pre><code>## Confusion Matrix and Statistics
## 
##           Reference
## Prediction bad good
##       bad   34   16
##       good  50  150
##                                           
##                Accuracy : 0.736           
##                  95% CI : (0.6768, 0.7895)
##     No Information Rate : 0.664           
##     P-Value [Acc &gt; NIR] : 0.008615        
##                                           
##                   Kappa : 0.3426          
##                                           
##  Mcnemar&#39;s Test P-Value : 4.865e-05       
##                                           
##             Sensitivity : 0.9036          
##             Specificity : 0.4048          
##          Pos Pred Value : 0.7500          
##          Neg Pred Value : 0.6800          
##              Prevalence : 0.6640          
##          Detection Rate : 0.6000          
##    Detection Prevalence : 0.8000          
##       Balanced Accuracy : 0.6542          
##                                           
##        &#39;Positive&#39; Class : good            
## </code></pre>
<p>Para obtener las estimaciones de las probabilidades, habría que establecer <code>type = &quot;probs&quot;</code> al predecir (devolverá una matriz con columnas correspondientes a los niveles):</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">p.est &lt;-<span class="st"> </span><span class="kw">predict</span>(ada.boost, <span class="dt">newdata =</span> test, <span class="dt">type =</span> <span class="st">&quot;probs&quot;</span>)
<span class="kw">head</span>(p.est)</code></pre></div>
<pre><code>##          [,1]      [,2]
## 1  0.49877103 0.5012290
## 4  0.30922187 0.6907781
## 9  0.02774336 0.9722566
## 10 0.04596187 0.9540381
## 12 0.44274407 0.5572559
## 16 0.37375910 0.6262409</code></pre>
<p>Este procedimiento también está implementado en el paquete <code>caret</code> seleccionando el método <code>&quot;ada&quot;</code>, que considera como hiperparámetros:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(caret)
<span class="kw">modelLookup</span>(<span class="st">&quot;ada&quot;</span>)</code></pre></div>
<pre><code>##   model parameter          label forReg forClass probModel
## 1   ada      iter         #Trees  FALSE     TRUE      TRUE
## 2   ada  maxdepth Max Tree Depth  FALSE     TRUE      TRUE
## 3   ada        nu  Learning Rate  FALSE     TRUE      TRUE</code></pre>
<p>Aunque por defecto la función <code>train()</code> solo considera nueve combinaciones de hiperparámetros:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">set.seed</span>(<span class="dv">1</span>)
caret.ada0 &lt;-<span class="st"> </span><span class="kw">train</span>(taste <span class="op">~</span><span class="st"> </span>., <span class="dt">method =</span> <span class="st">&quot;ada&quot;</span>, <span class="dt">data =</span> train,
                   <span class="dt">trControl =</span> <span class="kw">trainControl</span>(<span class="dt">method =</span> <span class="st">&quot;cv&quot;</span>, <span class="dt">number =</span> <span class="dv">5</span>))
caret.ada0</code></pre></div>
<pre><code>## Boosted Classification Trees 
## 
## 1000 samples
##   11 predictor
##    2 classes: &#39;bad&#39;, &#39;good&#39; 
## 
## No pre-processing
## Resampling: Cross-Validated (5 fold) 
## Summary of sample sizes: 800, 801, 800, 800, 799 
## Resampling results across tuning parameters:
## 
##   maxdepth  iter  Accuracy   Kappa    
##   1          50   0.7100121  0.2403486
##   1         100   0.7220322  0.2824931
##   1         150   0.7360322  0.3346624
##   2          50   0.7529774  0.3872880
##   2         100   0.7539673  0.4019619
##   2         150   0.7559673  0.4142035
##   3          50   0.7570024  0.4112842
##   3         100   0.7550323  0.4150030
##   3         150   0.7650024  0.4408835
## 
## Tuning parameter &#39;nu&#39; was held constant at a value of 0.1
## Accuracy was used to select the optimal model using the largest value.
## The final values used for the model were iter = 150, maxdepth = 3 and nu = 0.1.</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">confusionMatrix</span>(<span class="kw">predict</span>(caret.ada0, <span class="dt">newdata =</span> test), test<span class="op">$</span>taste, <span class="dt">positive =</span> <span class="st">&quot;good&quot;</span>)</code></pre></div>
<pre><code>## Confusion Matrix and Statistics
## 
##           Reference
## Prediction bad good
##       bad   37   22
##       good  47  144
##                                           
##                Accuracy : 0.724           
##                  95% CI : (0.6641, 0.7785)
##     No Information Rate : 0.664           
##     P-Value [Acc &gt; NIR] : 0.024724        
##                                           
##                   Kappa : 0.3324          
##                                           
##  Mcnemar&#39;s Test P-Value : 0.003861        
##                                           
##             Sensitivity : 0.8675          
##             Specificity : 0.4405          
##          Pos Pred Value : 0.7539          
##          Neg Pred Value : 0.6271          
##              Prevalence : 0.6640          
##          Detection Rate : 0.5760          
##    Detection Prevalence : 0.7640          
##       Balanced Accuracy : 0.6540          
##                                           
##        &#39;Positive&#39; Class : good            
## </code></pre>
<p>Se puede aumentar el número de combinaciones empleando <code>tuneLength</code> o <code>tuneGrid</code> pero la búsqueda en una rejilla completa puede incrementar considerablemente el tiempo de computación. Por este motivo se suelen seguir distintos procedimientos de búsqueda. Por ejemplo, fijar la tasa de aprendizaje (inicialmente a un valor alto) para seleccionar primero un número de interaciones y la complejidad del árbol, y posteriormente fijar estos valores para seleccionar una nueva tasa de aprendizaje (repitiendo el proceso, si es necesario, hasta convergencia).</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">set.seed</span>(<span class="dv">1</span>)
caret.ada1 &lt;-<span class="st"> </span><span class="kw">train</span>(taste <span class="op">~</span><span class="st"> </span>., <span class="dt">method =</span> <span class="st">&quot;ada&quot;</span>, <span class="dt">data =</span> train,
                    <span class="dt">tuneGrid =</span> <span class="kw">data.frame</span>(<span class="dt">iter =</span>  <span class="dv">150</span>, <span class="dt">maxdepth =</span> <span class="dv">3</span>,
                                 <span class="dt">nu =</span> <span class="kw">c</span>(<span class="fl">0.3</span>, <span class="fl">0.1</span>, <span class="fl">0.05</span>, <span class="fl">0.01</span>, <span class="fl">0.005</span>)),
                   <span class="dt">trControl =</span> <span class="kw">trainControl</span>(<span class="dt">method =</span> <span class="st">&quot;cv&quot;</span>, <span class="dt">number =</span> <span class="dv">5</span>))
caret.ada1</code></pre></div>
<pre><code>## Boosted Classification Trees 
## 
## 1000 samples
##   11 predictor
##    2 classes: &#39;bad&#39;, &#39;good&#39; 
## 
## No pre-processing
## Resampling: Cross-Validated (5 fold) 
## Summary of sample sizes: 800, 801, 800, 800, 799 
## Resampling results across tuning parameters:
## 
##   nu     Accuracy   Kappa    
##   0.005  0.7439722  0.3723405
##   0.010  0.7439822  0.3725968
##   0.050  0.7559773  0.4116753
##   0.100  0.7619774  0.4365242
##   0.300  0.7580124  0.4405127
## 
## Tuning parameter &#39;iter&#39; was held constant at a value of 150
## Tuning
##  parameter &#39;maxdepth&#39; was held constant at a value of 3
## Accuracy was used to select the optimal model using the largest value.
## The final values used for the model were iter = 150, maxdepth = 3 and nu = 0.1.</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">confusionMatrix</span>(<span class="kw">predict</span>(caret.ada1, <span class="dt">newdata =</span> test), test<span class="op">$</span>taste, <span class="dt">positive =</span> <span class="st">&quot;good&quot;</span>)</code></pre></div>
<pre><code>## Confusion Matrix and Statistics
## 
##           Reference
## Prediction bad good
##       bad   40   21
##       good  44  145
##                                          
##                Accuracy : 0.74           
##                  95% CI : (0.681, 0.7932)
##     No Information Rate : 0.664          
##     P-Value [Acc &gt; NIR] : 0.005841       
##                                          
##                   Kappa : 0.375          
##                                          
##  Mcnemar&#39;s Test P-Value : 0.006357       
##                                          
##             Sensitivity : 0.8735         
##             Specificity : 0.4762         
##          Pos Pred Value : 0.7672         
##          Neg Pred Value : 0.6557         
##              Prevalence : 0.6640         
##          Detection Rate : 0.5800         
##    Detection Prevalence : 0.7560         
##       Balanced Accuracy : 0.6748         
##                                          
##        &#39;Positive&#39; Class : good           
## </code></pre>
</div>
<div id="ejemplo-regresión-con-el-paquete-gbm" class="section level3">
<h3><span class="header-section-number">3.5.2</span> Ejemplo: regresión con el paquete <code>gbm</code></h3>
<p>El paquete <a href="https://CRAN.R-project.org/package=gbm"><code>gbm</code></a> implementa el algoritmo SGB de Friedman (2002) y admite varios tipos de respuesta considerando distintas funciones de pérdida (aunque en el caso de variables dicotómicas éstas deben tomar valores en <span class="math inline">\(\{0, 1\}\)</span><a href="#fn20" class="footnoteRef" id="fnref20"><sup>20</sup></a>). La función principal es <code>gbm()</code> y se suelen considerar los siguientes argumentos:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">gbm</span>( formula, <span class="dt">distribution =</span> <span class="st">&quot;bernoulli&quot;</span>, data, <span class="dt">n.trees =</span> <span class="dv">100</span>, 
     <span class="dt">interaction.depth =</span> <span class="dv">1</span>, <span class="dt">n.minobsinnode =</span> <span class="dv">10</span>,
     <span class="dt">shrinkage =</span> <span class="fl">0.1</span>, <span class="dt">bag.fraction =</span> <span class="fl">0.5</span>, 
     <span class="dt">cv.folds =</span> <span class="dv">0</span>, <span class="dt">n.cores =</span> <span class="ot">NULL</span>)</code></pre></div>
<ul>
<li><p><code>formula</code> y <code>data</code> (opcional): permiten especificar la respuesta y las variables predictoras de la forma habitual (típicamente <code>respuesta ~ .</code>; también está disponible una interfaz con matrices <code>gbm.fit()</code>).</p></li>
<li><p><code>distribution</code> (opcional): texto con el nombre de la distribución (o lista con el nombre en <code>name</code> y parámetros adicionales en los demás componentes) que determina la función de pérdida. Si se omite se establecerá a partir del tipo de la respuesta: <code>&quot;bernouilli&quot;</code> (regresión logística) si es una variable dicotómica 0/1, <code>&quot;multinomial&quot;</code> (regresión multinomial) si es un factor (no se recomienda) y <code>&quot;gaussian&quot;</code> (error cuadrático) en caso contrario. Otras opciones que pueden ser de interés son: <code>&quot;laplace&quot;</code> (error absoluto), <code>&quot;adaboost&quot;</code> (pérdida exponencial para respuestas dicotómicas 0/1), <code>&quot;huberized&quot;</code> (pérdida de Huber para respuestas dicotómicas 0/1), <code>&quot;poisson&quot;</code> (regresión de Poisson) y <code>&quot;quantile&quot;</code> (regresión cuantil).</p></li>
<li><p><code>ntrees</code>: iteraciones/número de árboles que se crecerán; por defecto 100 (se puede emplear la función <code>gbm.perf()</code> para seleccionar un valor “óptimo”).</p></li>
<li><p><code>interaction.depth</code>: profundidad de los árboles; por defecto 1 (modelo aditivo).</p></li>
<li><p><code>n.minobsinnode</code>: número mínimo de observaciones en un nodo terminal; por defecto 10.</p></li>
<li><p><code>shrinkage</code>: parámetro de regularización <span class="math inline">\(\lambda\)</span>; por defecto 0.1.</p></li>
<li><p><code>bag.fraction</code>: proporción de observaciones seleccionadas al azar para crecer cada árbol; por defecto 0.5.</p></li>
<li><p><code>cv.folds</code>: número grupos para validación cruzada; por defecto 0 (no se hace validación cruzada). Si se asigna un valor mayor que 1 se realizará validación cruzada y se devolverá el error en la componente <code>$cv.error</code> (se puede emplear para seleccionar hiperparámetros).</p></li>
<li><p><code>n.cores</code>: número de núcleos para el procesamiento en paralelo.</p></li>
</ul>
<p>Como ejemplo consideraremos el conjunto de datos <em>winequality.RData</em>:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">load</span>(<span class="st">&quot;data/winequality.RData&quot;</span>)
<span class="kw">set.seed</span>(<span class="dv">1</span>)
df &lt;-<span class="st"> </span>winequality
nobs &lt;-<span class="st"> </span><span class="kw">nrow</span>(df)
itrain &lt;-<span class="st"> </span><span class="kw">sample</span>(nobs, <span class="fl">0.8</span> <span class="op">*</span><span class="st"> </span>nobs)
train &lt;-<span class="st"> </span>df[itrain, ]
test &lt;-<span class="st"> </span>df[<span class="op">-</span>itrain, ]

<span class="kw">library</span>(gbm)
gbm.fit &lt;-<span class="st"> </span><span class="kw">gbm</span>(quality <span class="op">~</span><span class="st"> </span>., <span class="dt">data =</span> train)</code></pre></div>
<pre><code>## Distribution not specified, assuming gaussian ...</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">gbm.fit</code></pre></div>
<pre><code>## gbm(formula = quality ~ ., data = train)
## A gradient boosted model with gaussian loss function.
## 100 iterations were performed.
## There were 11 predictors of which 11 had non-zero influence.</code></pre>
<p>El método <code>summary()</code> calcula las medidas de influencia de los predictores y las representa gráficamente:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">summary</span>(gbm.fit)</code></pre></div>
<p><img src="03-bagging_boosting_files/figure-html/unnamed-chunk-29-1.png" width="80%" style="display: block; margin: auto;" /></p>
<pre><code>##                                       var   rel.inf
## alcohol                           alcohol 40.907998
## volatile.acidity         volatile.acidity 13.839083
## free.sulfur.dioxide   free.sulfur.dioxide 11.488262
## fixed.acidity               fixed.acidity  7.914742
## citric.acid                   citric.acid  6.765875
## total.sulfur.dioxide total.sulfur.dioxide  4.808308
## residual.sugar             residual.sugar  4.758566
## chlorides                       chlorides  3.424537
## sulphates                       sulphates  3.086036
## density                           density  1.918442
## pH                                     pH  1.088152</code></pre>
<p>Para estudiar el efecto de un predictor se pueden gererar gráficos de los efectos parciales mediante el método <code>plot()</code>:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">plot</span>(gbm.fit, <span class="dt">i =</span> <span class="st">&quot;alcohol&quot;</span>)</code></pre></div>
<p><img src="03-bagging_boosting_files/figure-html/unnamed-chunk-30-1.png" width="80%" style="display: block; margin: auto;" /></p>
<p>Finalmente podemos evaluar la precisión en la muestra de test empleando el código habitual:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">pred &lt;-<span class="st"> </span><span class="kw">predict</span>(gbm.fit, <span class="dt">newdata =</span> test)
obs &lt;-<span class="st"> </span>test<span class="op">$</span>quality

<span class="co"># Con el paquete caret</span>
caret<span class="op">::</span><span class="kw">postResample</span>(pred, obs)</code></pre></div>
<pre><code>##      RMSE  Rsquared       MAE 
## 0.7586208 0.3001401 0.6110442</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Con la función accuracy()</span>
accuracy &lt;-<span class="st"> </span><span class="cf">function</span>(pred, obs, <span class="dt">na.rm =</span> <span class="ot">FALSE</span>,
                     <span class="dt">tol =</span> <span class="kw">sqrt</span>(.Machine<span class="op">$</span>double.eps)) {
  err &lt;-<span class="st"> </span>obs <span class="op">-</span><span class="st"> </span>pred     <span class="co"># Errores</span>
  <span class="cf">if</span>(na.rm) {
    is.a &lt;-<span class="st"> </span><span class="op">!</span><span class="kw">is.na</span>(err)
    err &lt;-<span class="st"> </span>err[is.a]
    obs &lt;-<span class="st"> </span>obs[is.a]
  }
  perr &lt;-<span class="st"> </span><span class="dv">100</span><span class="op">*</span>err<span class="op">/</span><span class="kw">pmax</span>(obs, tol)  <span class="co"># Errores porcentuales</span>
  <span class="kw">return</span>(<span class="kw">c</span>(
    <span class="dt">me =</span> <span class="kw">mean</span>(err),           <span class="co"># Error medio</span>
    <span class="dt">rmse =</span> <span class="kw">sqrt</span>(<span class="kw">mean</span>(err<span class="op">^</span><span class="dv">2</span>)), <span class="co"># Raíz del error cuadrático medio</span>
    <span class="dt">mae =</span> <span class="kw">mean</span>(<span class="kw">abs</span>(err)),     <span class="co"># Error absoluto medio</span>
    <span class="dt">mpe =</span> <span class="kw">mean</span>(perr),         <span class="co"># Error porcentual medio</span>
    <span class="dt">mape =</span> <span class="kw">mean</span>(<span class="kw">abs</span>(perr)),   <span class="co"># Error porcentual absoluto medio</span>
    <span class="dt">r.squared =</span> <span class="dv">1</span> <span class="op">-</span><span class="st"> </span><span class="kw">sum</span>(err<span class="op">^</span><span class="dv">2</span>)<span class="op">/</span><span class="kw">sum</span>((obs <span class="op">-</span><span class="st"> </span><span class="kw">mean</span>(obs))<span class="op">^</span><span class="dv">2</span>)
  ))
}
<span class="kw">accuracy</span>(pred, obs)</code></pre></div>
<pre><code>##          me        rmse         mae         mpe        mape   r.squared 
## -0.01463661  0.75862081  0.61104421 -2.00702056 10.69753668  0.29917590</code></pre>
<!-- 
Pendiente: ejercicio regresión con el conjunto de datos Boston empleando error absoluto para evitar la influencia de datos atípicos. 
-->
<p>Este procedimiento también está implementado en el paquete <code>caret</code> seleccionando el método <code>&quot;gbm&quot;</code>, que considera como hiperparámetros:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(caret)
<span class="kw">modelLookup</span>(<span class="st">&quot;gbm&quot;</span>)</code></pre></div>
<pre><code>##   model         parameter                   label forReg forClass probModel
## 1   gbm           n.trees   # Boosting Iterations   TRUE     TRUE      TRUE
## 2   gbm interaction.depth          Max Tree Depth   TRUE     TRUE      TRUE
## 3   gbm         shrinkage               Shrinkage   TRUE     TRUE      TRUE
## 4   gbm    n.minobsinnode Min. Terminal Node Size   TRUE     TRUE      TRUE</code></pre>
<p>Aunque por defecto la función <code>train()</code> solo considera nueve combinaciones de hiperparámetros. Para hacer una búsqueda más completa se podría seguir un procedimiento análogo al empleado con el método anterior:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">set.seed</span>(<span class="dv">1</span>)
caret.gbm0 &lt;-<span class="st"> </span><span class="kw">train</span>(quality <span class="op">~</span><span class="st"> </span>., <span class="dt">method =</span> <span class="st">&quot;gbm&quot;</span>, <span class="dt">data =</span> train,
                   <span class="dt">trControl =</span> <span class="kw">trainControl</span>(<span class="dt">method =</span> <span class="st">&quot;cv&quot;</span>, <span class="dt">number =</span> <span class="dv">5</span>))</code></pre></div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">caret.gbm0</code></pre></div>
<pre><code>## Stochastic Gradient Boosting 
## 
## 1000 samples
##   11 predictor
## 
## No pre-processing
## Resampling: Cross-Validated (5 fold) 
## Summary of sample sizes: 800, 801, 800, 800, 799 
## Resampling results across tuning parameters:
## 
##   interaction.depth  n.trees  RMSE       Rsquared   MAE      
##   1                   50      0.7464098  0.2917796  0.5949686
##   1                  100      0.7258319  0.3171046  0.5751816
##   1                  150      0.7247246  0.3197241  0.5719404
##   2                   50      0.7198195  0.3307665  0.5712468
##   2                  100      0.7175006  0.3332903  0.5647409
##   2                  150      0.7258174  0.3222006  0.5713116
##   3                   50      0.7241661  0.3196365  0.5722590
##   3                  100      0.7272094  0.3191252  0.5754363
##   3                  150      0.7311429  0.3152905  0.5784988
## 
## Tuning parameter &#39;shrinkage&#39; was held constant at a value of 0.1
## 
## Tuning parameter &#39;n.minobsinnode&#39; was held constant at a value of 10
## RMSE was used to select the optimal model using the smallest value.
## The final values used for the model were n.trees = 100, interaction.depth =
##  2, shrinkage = 0.1 and n.minobsinnode = 10.</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">caret.gbm1 &lt;-<span class="st"> </span><span class="kw">train</span>(quality <span class="op">~</span><span class="st"> </span>., <span class="dt">method =</span> <span class="st">&quot;gbm&quot;</span>, <span class="dt">data =</span> train,
   <span class="dt">tuneGrid =</span> <span class="kw">data.frame</span>(<span class="dt">n.trees =</span>  <span class="dv">100</span>, <span class="dt">interaction.depth =</span> <span class="dv">2</span>, 
                        <span class="dt">shrinkage =</span> <span class="kw">c</span>(<span class="fl">0.3</span>, <span class="fl">0.1</span>, <span class="fl">0.05</span>, <span class="fl">0.01</span>, <span class="fl">0.005</span>),
                        <span class="dt">n.minobsinnode =</span> <span class="dv">10</span>),
   <span class="dt">trControl =</span> <span class="kw">trainControl</span>(<span class="dt">method =</span> <span class="st">&quot;cv&quot;</span>, <span class="dt">number =</span> <span class="dv">5</span>))</code></pre></div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">caret.gbm1</code></pre></div>
<pre><code>## Stochastic Gradient Boosting 
## 
## 1000 samples
##   11 predictor
## 
## No pre-processing
## Resampling: Cross-Validated (5 fold) 
## Summary of sample sizes: 800, 800, 801, 799, 800 
## Resampling results across tuning parameters:
## 
##   shrinkage  RMSE       Rsquared   MAE      
##   0.005      0.8154916  0.2419131  0.6245818
##   0.010      0.7844257  0.2602989  0.6128582
##   0.050      0.7206972  0.3275463  0.5707273
##   0.100      0.7124838  0.3407642  0.5631748
##   0.300      0.7720844  0.2613835  0.6091765
## 
## Tuning parameter &#39;n.trees&#39; was held constant at a value of 100
## Tuning
##  parameter &#39;interaction.depth&#39; was held constant at a value of 2
## 
## Tuning parameter &#39;n.minobsinnode&#39; was held constant at a value of 10
## RMSE was used to select the optimal model using the smallest value.
## The final values used for the model were n.trees = 100, interaction.depth =
##  2, shrinkage = 0.1 and n.minobsinnode = 10.</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">varImp</span>(caret.gbm1)</code></pre></div>
<pre><code>## gbm variable importance
## 
##                       Overall
## alcohol              100.0000
## volatile.acidity      28.4909
## free.sulfur.dioxide   24.5158
## residual.sugar        16.8406
## fixed.acidity         12.5623
## density               10.1917
## citric.acid            9.1542
## total.sulfur.dioxide   7.2659
## chlorides              4.5106
## pH                     0.1096
## sulphates              0.0000</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">postResample</span>(<span class="kw">predict</span>(caret.gbm1, <span class="dt">newdata =</span> test), test<span class="op">$</span>quality)</code></pre></div>
<pre><code>##      RMSE  Rsquared       MAE 
## 0.7403768 0.3329751 0.6017281</code></pre>
</div>
<div id="ejemplo-xgboost-con-el-paquete-caret" class="section level3">
<h3><span class="header-section-number">3.5.3</span> Ejemplo: XGBoost con el paquete <code>caret</code></h3>
<p>El método boosting implementado en el paquete <a href="https://github.com/dmlc/xgboost/tree/master/R-package"><code>xgboost</code></a> es uno de los más populares hoy en día. Esta implementación proporciona parámetros adicionales de regularización para controlar la complejidad del modelo y tratar de evitar el sobreajuste. También incluye criterios de parada, para detener la evaluación del modelo cuando los árboles adicionales no ofrecen ninguna mejora. Dispone de una interfaz simple <code>xgboost()</code> y otra más avanzada <code>xgb.train()</code>, que admite funciones de pérdida y evaluación personalizadas. Normalmente es necesario un preprocesado de los datos antes de llamar a estas funciones, ya que requieren de una matriz para los predictores y de un vector para la respuesta (además en el caso de que sea dicotómica debe tomar valores en <span class="math inline">\(\{0, 1\}\)</span>). Por tanto es necesario recodificar las variables categóricas como numéricas. Por este motivo puede ser preferible emplear la interfaz de <code>caret</code>.</p>
<p>El algoritmo estándar <em>XGBoost</em>, que emplea árboles como modelo base, está implementado en el método <code>&quot;xgbTree&quot;</code> de <code>caret</code><a href="#fn21" class="footnoteRef" id="fnref21"><sup>21</sup></a>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(caret)
<span class="co"># names(getModelInfo(&quot;xgb&quot;))</span>
<span class="kw">modelLookup</span>(<span class="st">&quot;xgbTree&quot;</span>)</code></pre></div>
<pre><code>##     model        parameter                          label forReg forClass
## 1 xgbTree          nrounds          # Boosting Iterations   TRUE     TRUE
## 2 xgbTree        max_depth                 Max Tree Depth   TRUE     TRUE
## 3 xgbTree              eta                      Shrinkage   TRUE     TRUE
## 4 xgbTree            gamma         Minimum Loss Reduction   TRUE     TRUE
## 5 xgbTree colsample_bytree     Subsample Ratio of Columns   TRUE     TRUE
## 6 xgbTree min_child_weight Minimum Sum of Instance Weight   TRUE     TRUE
## 7 xgbTree        subsample           Subsample Percentage   TRUE     TRUE
##   probModel
## 1      TRUE
## 2      TRUE
## 3      TRUE
## 4      TRUE
## 5      TRUE
## 6      TRUE
## 7      TRUE</code></pre>
<p>Este método considera los siguientes hiperparámetros:</p>
<ul>
<li><p><code>&quot;nrounds&quot;</code>: número de iteraciones boosting.</p></li>
<li><p><code>&quot;max_depth&quot;</code>: profundidad máxima del árbol; por defecto 6.</p></li>
<li><p><code>&quot;eta&quot;</code>: parámetro de regularización <span class="math inline">\(\lambda\)</span>; por defecto 0.3.</p></li>
<li><p><code>&quot;gamma&quot;</code>: mínima reducción de la pérdida para hacer una partición adicional en un nodo del árbol; por defecto 0.</p></li>
<li><p><code>&quot;colsample_bytree&quot;</code>: proporción de predictores seleccionados al azar para crecer cada árbol; por defecto 1.</p></li>
<li><p><code>&quot;min_child_weight&quot;</code>: suma mínima de peso (hessiana) para hacer una partición adicional en un nodo del árbol; por defecto 1.</p></li>
<li><p><code>&quot;subsample&quot;</code>: proporción de observaciones seleccionadas al azar en cada iteración boosting; por defecto 1.</p></li>
</ul>
<p>Para más información sobre parámetros adicionales se puede consultar la ayuda de <code>xgboost::xgboost()</code> o la lista detallada disponible en la Sección <a href="https://xgboost.readthedocs.io/en/latest/parameter.html">XGBoost Parameters</a> del <a href="https://xgboost.readthedocs.io">Manual de XGBoost</a>.</p>
<p>Como ejemplo consideraremos el problema de clasificación empleando el conjunto de datos de calidad de vino:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">load</span>(<span class="st">&quot;data/winetaste.RData&quot;</span>)
<span class="kw">set.seed</span>(<span class="dv">1</span>)
df &lt;-<span class="st"> </span>winetaste
nobs &lt;-<span class="st"> </span><span class="kw">nrow</span>(df)
itrain &lt;-<span class="st"> </span><span class="kw">sample</span>(nobs, <span class="fl">0.8</span> <span class="op">*</span><span class="st"> </span>nobs)
train &lt;-<span class="st"> </span>df[itrain, ]
test &lt;-<span class="st"> </span>df[<span class="op">-</span>itrain, ]</code></pre></div>
<p>En este caso la función <code>train()</code> considera por defecto 108 combinaciones de hiperparámetros y el tiempo de computación puede ser excesivo.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">caret.xgb &lt;-<span class="st"> </span><span class="kw">train</span>(taste <span class="op">~</span><span class="st"> </span>., <span class="dt">method =</span> <span class="st">&quot;xgbTree&quot;</span>, <span class="dt">data =</span> train,
                   <span class="dt">trControl =</span> <span class="kw">trainControl</span>(<span class="dt">method =</span> <span class="st">&quot;cv&quot;</span>, <span class="dt">number =</span> <span class="dv">5</span>))
caret.xgb</code></pre></div>
<pre><code>## eXtreme Gradient Boosting 
## 
## 1000 samples
##   11 predictor
##    2 classes: &#39;good&#39;, &#39;bad&#39; 
## 
## No pre-processing
## Resampling: Cross-Validated (5 fold) 
## Summary of sample sizes: 799, 801, 801, 799, 800 
## Resampling results across tuning parameters:
## 
##   eta  max_depth  colsample_bytree  subsample  nrounds  Accuracy   Kappa    
##   0.3  1          0.6               0.50        50      0.7270047  0.3456824
##   0.3  1          0.6               0.50       100      0.7400300  0.3856840
##   0.3  1          0.6               0.50       150      0.7380249  0.3849720
##   0.3  1          0.6               0.75        50      0.7429749  0.3833619
##   0.3  1          0.6               0.75       100      0.7369147  0.3839315
##   0.3  1          0.6               0.75       150      0.7409547  0.3994590
##   0.3  1          0.6               1.00        50      0.7429599  0.3774907
##   0.3  1          0.6               1.00       100      0.7469350  0.4004720
##   0.3  1          0.6               1.00       150      0.7429749  0.3922574
##   0.3  1          0.8               0.50        50      0.7449400  0.3928409
##   0.3  1          0.8               0.50       100      0.7269846  0.3594183
##   0.3  1          0.8               0.50       150      0.7319947  0.3667027
##   0.3  1          0.8               0.75        50      0.7359497  0.3727139
##   0.3  1          0.8               0.75       100      0.7409747  0.3949485
##   0.3  1          0.8               0.75       150      0.7329645  0.3759044
##   0.3  1          0.8               1.00        50      0.7429449  0.3799009
##   0.3  1          0.8               1.00       100      0.7469450  0.3978185
##   0.3  1          0.8               1.00       150      0.7509350  0.4094308
##   0.3  2          0.6               0.50        50      0.7399448  0.3879917
##   0.3  2          0.6               0.50       100      0.7349197  0.3871037
##   0.3  2          0.6               0.50       150      0.7269546  0.3737454
##   0.3  2          0.6               0.75        50      0.7409597  0.3945359
##   0.3  2          0.6               0.75       100      0.7399998  0.3941711
##   0.3  2          0.6               0.75       150      0.7339997  0.3877357
##   0.3  2          0.6               1.00        50      0.7499299  0.4101402
##   0.3  2          0.6               1.00       100      0.7399147  0.3904076
##   0.3  2          0.6               1.00       150      0.7479700  0.4167337
##   0.3  2          0.8               0.50        50      0.7580353  0.4210483
##   0.3  2          0.8               0.50       100      0.7509800  0.4177512
##   0.3  2          0.8               0.50       150      0.7389548  0.3907023
##   0.3  2          0.8               0.75        50      0.7429748  0.4030924
##   0.3  2          0.8               0.75       100      0.7469849  0.4152931
##   0.3  2          0.8               0.75       150      0.7510300  0.4267509
##   0.3  2          0.8               1.00        50      0.7520050  0.4166915
##   0.3  2          0.8               1.00       100      0.7520151  0.4249264
##   0.3  2          0.8               1.00       150      0.7589952  0.4401835
##   0.3  3          0.6               0.50        50      0.7380496  0.3916075
##   0.3  3          0.6               0.50       100      0.7540151  0.4330095
##   0.3  3          0.6               0.50       150      0.7369746  0.3892166
##   0.3  3          0.6               0.75        50      0.7589451  0.4350590
##   0.3  3          0.6               0.75       100      0.7500400  0.4217787
##   0.3  3          0.6               0.75       150      0.7519848  0.4246740
##   0.3  3          0.6               1.00        50      0.7529350  0.4162968
##   0.3  3          0.6               1.00       100      0.7589451  0.4391310
##   0.3  3          0.6               1.00       150      0.7499649  0.4215876
##   0.3  3          0.8               0.50        50      0.7649753  0.4550539
##   0.3  3          0.8               0.50       100      0.7569902  0.4351073
##   0.3  3          0.8               0.50       150      0.7499250  0.4210503
##   0.3  3          0.8               0.75        50      0.7589802  0.4396966
##   0.3  3          0.8               0.75       100      0.7599901  0.4439201
##   0.3  3          0.8               0.75       150      0.7559950  0.4396551
##   0.3  3          0.8               1.00        50      0.7529700  0.4270112
##   0.3  3          0.8               1.00       100      0.7589401  0.4432280
##   0.3  3          0.8               1.00       150      0.7569302  0.4368301
##   0.4  1          0.6               0.50        50      0.7389297  0.3852019
##   0.4  1          0.6               0.50       100      0.7449749  0.4067440
##   0.4  1          0.6               0.50       150      0.7539900  0.4274551
##   0.4  1          0.6               0.75        50      0.7419399  0.3947505
##   0.4  1          0.6               0.75       100      0.7359697  0.3832840
##   0.4  1          0.6               0.75       150      0.7289397  0.3685824
##   0.4  1          0.6               1.00        50      0.7429399  0.3894412
##   0.4  1          0.6               1.00       100      0.7499700  0.4103344
##   0.4  1          0.6               1.00       150      0.7429999  0.3981893
##   0.4  1          0.8               0.50        50      0.7359897  0.3856013
##   0.4  1          0.8               0.50       100      0.7420099  0.4048545
##   0.4  1          0.8               0.50       150      0.7400048  0.4005212
##   0.4  1          0.8               0.75        50      0.7549702  0.4186415
##   0.4  1          0.8               0.75       100      0.7449700  0.4070417
##   0.4  1          0.8               0.75       150      0.7470200  0.4082903
##   0.4  1          0.8               1.00        50      0.7479300  0.3966813
##   0.4  1          0.8               1.00       100      0.7429648  0.3938550
##   0.4  1          0.8               1.00       150      0.7489650  0.4097744
##   0.4  2          0.6               0.50        50      0.7339647  0.3818445
##   0.4  2          0.6               0.50       100      0.7369097  0.3959654
##   0.4  2          0.6               0.50       150      0.7309445  0.3806622
##   0.4  2          0.6               0.75        50      0.7379998  0.3948626
##   0.4  2          0.6               0.75       100      0.7449999  0.4075786
##   0.4  2          0.6               0.75       150      0.7500052  0.4207846
##   0.4  2          0.6               1.00        50      0.7519450  0.4182109
##   0.4  2          0.6               1.00       100      0.7420148  0.4036651
##   0.4  2          0.6               1.00       150      0.7529999  0.4280194
##   0.4  2          0.8               0.50        50      0.7460450  0.4127948
##   0.4  2          0.8               0.50       100      0.7579951  0.4452801
##   0.4  2          0.8               0.50       150      0.7509600  0.4277173
##   0.4  2          0.8               0.75        50      0.7480299  0.4171753
##   0.4  2          0.8               0.75       100      0.7550400  0.4406821
##   0.4  2          0.8               0.75       150      0.7460399  0.4215027
##   0.4  2          0.8               1.00        50      0.7599751  0.4401484
##   0.4  2          0.8               1.00       100      0.7580353  0.4406080
##   0.4  2          0.8               1.00       150      0.7719806  0.4720566
##   0.4  3          0.6               0.50        50      0.7409748  0.4100874
##   0.4  3          0.6               0.50       100      0.7419948  0.4128674
##   0.4  3          0.6               0.50       150      0.7399998  0.4069074
##   0.4  3          0.6               0.75        50      0.7419649  0.4073670
##   0.4  3          0.6               0.75       100      0.7459499  0.4210145
##   0.4  3          0.6               0.75       150      0.7599252  0.4472654
##   0.4  3          0.6               1.00        50      0.7529949  0.4349779
##   0.4  3          0.6               1.00       100      0.7589702  0.4437452
##   0.4  3          0.6               1.00       150      0.7559600  0.4383594
##   0.4  3          0.8               0.50        50      0.7489800  0.4177928
##   0.4  3          0.8               0.50       100      0.7379797  0.4051258
##   0.4  3          0.8               0.50       150      0.7439547  0.4159864
##   0.4  3          0.8               0.75        50      0.7449899  0.4186248
##   0.4  3          0.8               0.75       100      0.7459549  0.4144890
##   0.4  3          0.8               0.75       150      0.7539601  0.4334041
##   0.4  3          0.8               1.00        50      0.7570301  0.4344584
##   0.4  3          0.8               1.00       100      0.7570152  0.4356542
##   0.4  3          0.8               1.00       150      0.7570201  0.4392371
## 
## Tuning parameter &#39;gamma&#39; was held constant at a value of 0
## Tuning
##  parameter &#39;min_child_weight&#39; was held constant at a value of 1
## Accuracy was used to select the optimal model using the largest value.
## The final values used for the model were nrounds = 150, max_depth = 2, eta
##  = 0.4, gamma = 0, colsample_bytree = 0.8, min_child_weight = 1 and subsample
##  = 1.</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">caret.xgb<span class="op">$</span>bestTune</code></pre></div>
<pre><code>##    nrounds max_depth eta gamma colsample_bytree min_child_weight subsample
## 90     150         2 0.4     0              0.8                1         1</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">varImp</span>(caret.xgb)</code></pre></div>
<pre><code>## xgbTree variable importance
## 
##                      Overall
## alcohol              100.000
## density               41.572
## citric.acid           39.002
## residual.sugar        33.656
## free.sulfur.dioxide   33.142
## volatile.acidity      31.888
## fixed.acidity         17.278
## total.sulfur.dioxide  14.832
## sulphates              7.958
## pH                     5.857
## chlorides              0.000</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">confusionMatrix</span>(<span class="kw">predict</span>(caret.xgb, <span class="dt">newdata =</span> test), test<span class="op">$</span>taste)</code></pre></div>
<pre><code>## Confusion Matrix and Statistics
## 
##           Reference
## Prediction good bad
##       good  145  43
##       bad    21  41
##                                           
##                Accuracy : 0.744           
##                  95% CI : (0.6852, 0.7969)
##     No Information Rate : 0.664           
##     P-Value [Acc &gt; NIR] : 0.003886        
##                                           
##                   Kappa : 0.3866          
##                                           
##  Mcnemar&#39;s Test P-Value : 0.008665        
##                                           
##             Sensitivity : 0.8735          
##             Specificity : 0.4881          
##          Pos Pred Value : 0.7713          
##          Neg Pred Value : 0.6613          
##              Prevalence : 0.6640          
##          Detection Rate : 0.5800          
##    Detection Prevalence : 0.7520          
##       Balanced Accuracy : 0.6808          
##                                           
##        &#39;Positive&#39; Class : good            
## </code></pre>
<p>Se podría seguir una estrategia de búsqueda similar a la empleada en los métodos anteriores.</p>

</div>
</div>
<!-- </div> -->
<div class="footnotes">
<hr />
<ol start="20">
<li id="fn20"><p>Se puede evitar este inconveniente empleando la interfaz de <code>caret</code>.<a href="boosting-en-r.html#fnref20">↩</a></p></li>
<li id="fn21"><p>Otras alternativas son: <code>&quot;xgbDART&quot;</code> que también emplean árboles como modelo base, pero incluye el método DART (Vinayak y Gilad-Bachrach, 2015) para evitar sobreajuste (básicamente descarta árboles al azar en la secuencia), y<code>&quot;xgbLinear&quot;</code> que emplea modelos lineales.<a href="boosting-en-r.html#fnref21">↩</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="boosting.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="referencias.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": true,
"facebook": false,
"twitter": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/rubenfcasal/aprendizaje_estadistico/edit/master/03-bagging_boosting.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["aprendizaje_estadistico.pdf"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
