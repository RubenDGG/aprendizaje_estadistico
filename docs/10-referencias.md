# Referencias {-}

<!-- 
# Referencias {-}
-->

## Bibliografía básica {-}

James, G., Witten, D., Hastie, T. y Tibshirani, R. (2013). *[An Introduction to Statistical Learning: with Aplications in R](http://faculty.marshall.usc.edu/gareth-james/ISL)*. Springer.

Kuhn, M. y Johnson, K. (2013). *[Applied predictive modeling](http://appliedpredictivemodeling.com)*. Springer.

Williams, G. (2011). *Data Mining with Rattle and R*. Springer.


## Bibliografía complementaria {-}

### Libros {-}

Bellman, R.E. (1961). *Adaptive Control Processes*, Princeton University Press.

Burger, S.V. (2018). *Introduction to machine learning with R: Rigorous mathematical analysis*. O'Reilly.

Breiman, L., Friedman, J., Stone, C.J. y Olshen, R.A. (1984). *Classification and regression trees*. CRC press.

Efron, B. y Hastie, T. (2016). *[Computer age statistical inference](http://web.stanford.edu/~hastie/CASI/)*. Cambridge University Press.

Fernández-Casal, R. y Cao, R. (2020). *Simulación Estadística*. <https://rubenfcasal.github.io/simbook>.

Hastie, T., Tibshirani, R. y Friedman, J. (2009).
    *[The Elements of Statistical Learning: Data Mining, Inference, and Prediction](https://web.stanford.edu/~hastie/ElemStatLearn)*. Springer.   
    
Hastie, T., Tibshirani, R. y Wainwright, M. (2015). *Statistical learning with sparsity: the lasso and generalizations*. CRC press.    

Irizarry, R.A. (2019). *[Introduction to Data Science: Data Analysis and Prediction Algorithms with R](https://rafalab.github.io/dsbook)*. CRC Press.

Molnar, C. (2020). [Interpretable Machine Learning. A Guide for Making Black Box Models Explainable](https://christophm.github.io/interpretable-ml-book). Lulu.com.

Torgo, L. (2011). *Data Mining with R: Learning with Case Studies*. Chapman & Hall/CRC Press.    


### Artículos {-}

Agor, J. y Özaltın, O.Y. (2019). Feature selection for classification models via bilevel optimization. *Computers & Operations Research*, 106, 156-168.

Biecek, P. (2018). [DALEX: explainers for complex predictive models in R](http://www.jmlr.org/papers/volume19/18-416/18-416.pdf). *The Journal of Machine Learning Research*, 19(1), 3245-3249.

Breiman, L. (1996). Bagging predictors. *Machine Learning*, 24(2), 123-140. 

Breiman, L. (2001). Random Forests. *Machine Learning*, 45(1), 5–32.

Breiman, L. (2001). Statistical Modeling: The Two Cultures (with comments and a rejoinder by the author). *Statistical Science*, 16, 199-231.

Culp, M., Johnson, K. y Michailidis, G. (2006). [ada: an R Package for Stochastic Boosting](https://www.jstatsoft.org/article/view/v017i02). *Journal of Statistical Software*, 17(2), 1-27.

Dietterich, T.G. (2000). An Experimental Comparison of Three Methods for Constructing Ensembles of Decision Trees: Bagging, Boosting, and Randomization. *Machine Learning*, 40(2), 139–158.

Dunson D.B. (2018). Statistics in the big data era: Failures of the machine. *Statistics and Probability Letters*, 136, 4-9.

Fisher, R.A. (1936). The use of multiple measurements in taxonomic problems. *Annals of Eugenics*, 7(2), 179-188.

Freund, Y. y Schapire, R. (1996). Experiments with a New Boosting Algorithm. *Machine Learning: Proceedings of the Thirteenth International Conference*, 148–156.

Friedman, J.H. (2001). [Greedy Function Approximation: A Gradient Boosting Machine](https://projecteuclid.org/euclid.aos/1013203451). *Annals of Statistics*, 29, 1189–1232.

Friedman, J.H. (2002). [Stochastic Gradient Boosting](https://www.sciencedirect.com/science/article/pii/S0167947301000652). *Computational Statistics and Data Analysis*, 38(4), 367-378. 

Friedman, J., Hastie, T. y Tibshirani, R. (2000). Additive Logistic Regression: A statistical view of boosting. *Annals of Statistics*, 28(2), 337-374. 

Friedman, J.H. y Popescu, B.E. (2008). Predictive learning via rule ensembles. *The Annals of Applied Statistics*, 2(3), 916-954. 
Goldstein, A., Kapelner, A., Bleich, J. y Pitkin, E. (2015). [Peeking inside the black box: Visualizing statistical learning with plots of individual conditional expectation](https://doi.org/10.1080/10618600.2014.907095). *Journal of Computational and Graphical Statistics*, 24(1), 44-65.

Greenwell, B.M. (2017). [pdp: An R Package for Constructing Partial Dependence Plots](https://journal.r-project.org/archive/2017/RJ-2017-016/index.html). *The R Journal*, 9(1), 421-436.

Kearns, M. y Valiant, L. (1989). Cryptographic Limitations on Learning Boolean Formulae and Finite Automata. *Proceedings of the Twenty-First Annual ACM Symposium on Theory of Computing*.

Kuhn, M. (2008). Building predictive models in R using the caret package. *Journal of Statistical Software*, 28(5), 1-26.

Lauro, C. (1996). Computational statistics or statistical computing, is that the question?, *Computational Statistics & Data Analysis*, 23 (1), 191-193.

Liaw, A. y Wiener, M. (2002). [Classification and regression by randomForest](https://www.r-project.org/doc/Rnews/Rnews_2002-3.pdf). *R News*, 2(3), 18-22.

Loh, W.Y. (2002). Regression tress with unbiased variable selection and interaction detection. *Statistica Sinica*, 361-386.

Shannon C (1948). A Mathematical Theory of Communication. *The Bell System Technical Journal*, 27, 379–423.

Strumbelj, E. y Kononenko, I. (2010). An efficient explanation of individual classifications using game theory. *Journal of Machine Learning Research*, 11, 1-18.

Valiant, L. (1984). A Theory of the Learnable. *Communications of the ACM*, 27, 1134–1142.

Vinayak, R.K. y Gilad-Bachrach, R. (2015). Dart: Dropouts meet multiple additive regression trees. *Proceedings of Machine Learning Research*, 38, 489-497.


<!-- 
Pendiente: Citar paquetes 

  citation("caret")
  Max Kuhn (2020). caret: Classification and Regression Training. R package version 6.0-86. https://CRAN.R-project.org/package=caret
  
  Emplear herramientas de bookdown

-->
