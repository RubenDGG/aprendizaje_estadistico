<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>1.6 Introducción al paquete caret | Aprendizaje Estadístico</title>
  <meta name="description" content="Apuntes de la asignatura de Aprendizaje Estadístico del Máster en Técnicas Estadísticas." />
  <meta name="generator" content="bookdown 0.18 and GitBook 2.6.7" />

  <meta property="og:title" content="1.6 Introducción al paquete caret | Aprendizaje Estadístico" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="Apuntes de la asignatura de Aprendizaje Estadístico del Máster en Técnicas Estadísticas." />
  <meta name="github-repo" content="rubenfcasal/aprendizaje_estadistico" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="1.6 Introducción al paquete caret | Aprendizaje Estadístico" />
  
  <meta name="twitter:description" content="Apuntes de la asignatura de Aprendizaje Estadístico del Máster en Técnicas Estadísticas." />
  

<meta name="author" content="Rubén Fernández Casal (ruben.fcasal@udc.es), Julián Costa (julian.costa@udc.es)" />


<meta name="date" content="2020-10-06" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="análisis-e-interpretación-de-los-modelos.html"/>
<link rel="next" href="trees.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />











<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Aprendizaje Estadístico</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Prólogo</a></li>
<li class="chapter" data-level="1" data-path="intro-AE.html"><a href="intro-AE.html"><i class="fa fa-check"></i><b>1</b> Introducción al Aprendizaje Estadístico</a><ul>
<li class="chapter" data-level="1.1" data-path="aprendizaje-estadístico-vs-aprendizaje-automático.html"><a href="aprendizaje-estadístico-vs-aprendizaje-automático.html"><i class="fa fa-check"></i><b>1.1</b> Aprendizaje Estadístico vs. Aprendizaje Automático</a><ul>
<li class="chapter" data-level="1.1.1" data-path="aprendizaje-estadístico-vs-aprendizaje-automático.html"><a href="aprendizaje-estadístico-vs-aprendizaje-automático.html#machine-learning-vs.data-mining"><i class="fa fa-check"></i><b>1.1.1</b> Machine Learning vs. Data Mining</a></li>
<li class="chapter" data-level="1.1.2" data-path="aprendizaje-estadístico-vs-aprendizaje-automático.html"><a href="aprendizaje-estadístico-vs-aprendizaje-automático.html#las-dos-culturas-breiman-2001"><i class="fa fa-check"></i><b>1.1.2</b> Las dos culturas (Breiman, 2001)</a></li>
<li class="chapter" data-level="1.1.3" data-path="aprendizaje-estadístico-vs-aprendizaje-automático.html"><a href="aprendizaje-estadístico-vs-aprendizaje-automático.html#machine-learning-vs.estadística-dunson-2018"><i class="fa fa-check"></i><b>1.1.3</b> Machine Learning vs. Estadística (Dunson, 2018)</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="métodos-de-aprendizaje-estadístico.html"><a href="métodos-de-aprendizaje-estadístico.html"><i class="fa fa-check"></i><b>1.2</b> Métodos de Aprendizaje Estadístico</a><ul>
<li class="chapter" data-level="1.2.1" data-path="métodos-de-aprendizaje-estadístico.html"><a href="métodos-de-aprendizaje-estadístico.html#notacion"><i class="fa fa-check"></i><b>1.2.1</b> Notación y terminología</a></li>
<li class="chapter" data-level="1.2.2" data-path="métodos-de-aprendizaje-estadístico.html"><a href="métodos-de-aprendizaje-estadístico.html#metodos-pkgs"><i class="fa fa-check"></i><b>1.2.2</b> Métodos (de aprendizaje supervisado) y paquetes de R</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="const-eval.html"><a href="const-eval.html"><i class="fa fa-check"></i><b>1.3</b> Construcción y evaluación de los modelos</a><ul>
<li class="chapter" data-level="1.3.1" data-path="const-eval.html"><a href="const-eval.html#bias-variance"><i class="fa fa-check"></i><b>1.3.1</b> Equilibrio entre sesgo y varianza: infraajuste y sobreajuste</a></li>
<li class="chapter" data-level="1.3.2" data-path="const-eval.html"><a href="const-eval.html#entrenamiento-test"><i class="fa fa-check"></i><b>1.3.2</b> Datos de entrenamiento y datos de test</a></li>
<li class="chapter" data-level="1.3.3" data-path="const-eval.html"><a href="const-eval.html#cv"><i class="fa fa-check"></i><b>1.3.3</b> Validación cruzada</a></li>
<li class="chapter" data-level="1.3.4" data-path="const-eval.html"><a href="const-eval.html#eval-reg"><i class="fa fa-check"></i><b>1.3.4</b> Evaluación de un método de regresión</a></li>
<li class="chapter" data-level="1.3.5" data-path="const-eval.html"><a href="const-eval.html#eval-class"><i class="fa fa-check"></i><b>1.3.5</b> Evaluación de un método de clasificación</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="la-maldición-de-la-dimensionalidad.html"><a href="la-maldición-de-la-dimensionalidad.html"><i class="fa fa-check"></i><b>1.4</b> La maldición de la dimensionalidad</a></li>
<li class="chapter" data-level="1.5" data-path="análisis-e-interpretación-de-los-modelos.html"><a href="análisis-e-interpretación-de-los-modelos.html"><i class="fa fa-check"></i><b>1.5</b> Análisis e interpretación de los modelos</a></li>
<li class="chapter" data-level="1.6" data-path="caret.html"><a href="caret.html"><i class="fa fa-check"></i><b>1.6</b> Introducción al paquete <code>caret</code></a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="trees.html"><a href="trees.html"><i class="fa fa-check"></i><b>2</b> Árboles de decisión</a><ul>
<li class="chapter" data-level="2.1" data-path="árboles-de-regresión-cart.html"><a href="árboles-de-regresión-cart.html"><i class="fa fa-check"></i><b>2.1</b> Árboles de regresión CART</a></li>
<li class="chapter" data-level="2.2" data-path="árboles-de-clasificación-cart.html"><a href="árboles-de-clasificación-cart.html"><i class="fa fa-check"></i><b>2.2</b> Árboles de clasificación CART</a></li>
<li class="chapter" data-level="2.3" data-path="cart-con-el-paquete-rpart.html"><a href="cart-con-el-paquete-rpart.html"><i class="fa fa-check"></i><b>2.3</b> CART con el paquete <code>rpart</code></a><ul>
<li class="chapter" data-level="2.3.1" data-path="cart-con-el-paquete-rpart.html"><a href="cart-con-el-paquete-rpart.html#ejemplo-regresión"><i class="fa fa-check"></i><b>2.3.1</b> Ejemplo: regresión</a></li>
<li class="chapter" data-level="2.3.2" data-path="cart-con-el-paquete-rpart.html"><a href="cart-con-el-paquete-rpart.html#ejemplo-modelo-de-clasificación"><i class="fa fa-check"></i><b>2.3.2</b> Ejemplo: modelo de clasificación</a></li>
<li class="chapter" data-level="2.3.3" data-path="cart-con-el-paquete-rpart.html"><a href="cart-con-el-paquete-rpart.html#interfaz-de-caret"><i class="fa fa-check"></i><b>2.3.3</b> Interfaz de <code>caret</code></a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="alternativas-a-los-árboles-cart.html"><a href="alternativas-a-los-árboles-cart.html"><i class="fa fa-check"></i><b>2.4</b> Alternativas a los árboles CART</a><ul>
<li class="chapter" data-level="2.4.1" data-path="alternativas-a-los-árboles-cart.html"><a href="alternativas-a-los-árboles-cart.html#ejemplo"><i class="fa fa-check"></i><b>2.4.1</b> Ejemplo</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="referencias.html"><a href="referencias.html"><i class="fa fa-check"></i>Referencias</a><ul>
<li class="chapter" data-level="" data-path="bibliografía-básica.html"><a href="bibliografía-básica.html"><i class="fa fa-check"></i>Bibliografía básica</a></li>
<li class="chapter" data-level="" data-path="bibliografía-complementaria.html"><a href="bibliografía-complementaria.html"><i class="fa fa-check"></i>Bibliografía complementaria</a><ul>
<li class="chapter" data-level="" data-path="bibliografía-complementaria.html"><a href="bibliografía-complementaria.html#libros"><i class="fa fa-check"></i>Libros</a></li>
<li class="chapter" data-level="" data-path="bibliografía-complementaria.html"><a href="bibliografía-complementaria.html#artículos"><i class="fa fa-check"></i>Artículos</a></li>
</ul></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Aprendizaje Estadístico</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="caret" class="section level2">
<h2><span class="header-section-number">1.6</span> Introducción al paquete <code>caret</code></h2>
<p>Como ya se comentó en la Sección <a href="métodos-de-aprendizaje-estadístico.html#metodos-pkgs">1.2.2</a>, el paquete <code>caret</code> (abreviatura de <em>Classification And REgression Training</em>) proporciona una interfaz unificada que simplifica el proceso de modelado empleando la mayoría de los métodos de AE implementados en R (actualmente admite 238 métodos; ver el <a href="https://topepo.github.io/caret/available-models.html">Capítulo 6</a> del <a href="https://topepo.github.io/caret">manual</a> de este paquete). Además de proporcionar rutinas para los principales pasos del proceso, incluye también numerosas funciones auxiliares que permitirían implementar nuevos procedimientos.</p>
<p>Enlaces:</p>
<ul>
<li><p><a href="https://topepo.github.io/caret">Manual</a></p>
<ul>
<li><p><a href="https://topepo.github.io/caret/pre-processing.html">3. Pre-Processing</a></p></li>
<li><p><a href="https://topepo.github.io/caret/model-training-and-tuning.html">5. Model Training and Tuning</a></p></li>
<li><p><a href="https://topepo.github.io/caret/available-models.html">6. Available Models</a></p></li>
<li><p><a href="https://topepo.github.io/caret/measuring-performance.html">17. Measuring Performance</a></p></li>
</ul></li>
<li><p><a href="https://cran.r-project.org/web/packages/caret/vignettes/caret.html">Vignette</a></p></li>
<li><p><a href="https://raw.githubusercontent.com/rstudio/cheatsheets/master/caret.pdf">Cheat Sheet</a></p></li>
</ul>
<p>La función principal es <code>train()</code> (descrita más adelante), que incluye un parámetro <code>method</code> que permite establecer el modelo mediante una cadena de texto. Podemos obtener información sobre los modelos disponibles con las funciones <code>getModelInfo()</code> y <code>modelLookup()</code> (puede haber varias implementaciones del mismo método con distintas configuraciones de hiperparámetros; también se pueden definir nuevos modelos, ver el <a href="https://topepo.github.io/caret/using-your-own-model-in-train.html">Capítulo 13</a> del <a href="https://topepo.github.io/caret">manual</a>).</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(caret)
<span class="kw">names</span>(<span class="kw">getModelInfo</span>()) <span class="co"># Listado de todos los métodos disponibles</span></code></pre></div>
<pre><code>##   [1] &quot;ada&quot;                 &quot;AdaBag&quot;              &quot;AdaBoost.M1&quot;        
##   [4] &quot;adaboost&quot;            &quot;amdai&quot;               &quot;ANFIS&quot;              
##   [7] &quot;avNNet&quot;              &quot;awnb&quot;                &quot;awtan&quot;              
##  [10] &quot;bag&quot;                 &quot;bagEarth&quot;            &quot;bagEarthGCV&quot;        
##  [13] &quot;bagFDA&quot;              &quot;bagFDAGCV&quot;           &quot;bam&quot;                
##  [16] &quot;bartMachine&quot;         &quot;bayesglm&quot;            &quot;binda&quot;              
##  [19] &quot;blackboost&quot;          &quot;blasso&quot;              &quot;blassoAveraged&quot;     
##  [22] &quot;bridge&quot;              &quot;brnn&quot;                &quot;BstLm&quot;              
##  [25] &quot;bstSm&quot;               &quot;bstTree&quot;             &quot;C5.0&quot;               
##  [28] &quot;C5.0Cost&quot;            &quot;C5.0Rules&quot;           &quot;C5.0Tree&quot;           
##  [31] &quot;cforest&quot;             &quot;chaid&quot;               &quot;CSimca&quot;             
##  [34] &quot;ctree&quot;               &quot;ctree2&quot;              &quot;cubist&quot;             
##  [37] &quot;dda&quot;                 &quot;deepboost&quot;           &quot;DENFIS&quot;             
##  [40] &quot;dnn&quot;                 &quot;dwdLinear&quot;           &quot;dwdPoly&quot;            
##  [43] &quot;dwdRadial&quot;           &quot;earth&quot;               &quot;elm&quot;                
##  [46] &quot;enet&quot;                &quot;evtree&quot;              &quot;extraTrees&quot;         
##  [49] &quot;fda&quot;                 &quot;FH.GBML&quot;             &quot;FIR.DM&quot;             
##  [52] &quot;foba&quot;                &quot;FRBCS.CHI&quot;           &quot;FRBCS.W&quot;            
##  [55] &quot;FS.HGD&quot;              &quot;gam&quot;                 &quot;gamboost&quot;           
##  [58] &quot;gamLoess&quot;            &quot;gamSpline&quot;           &quot;gaussprLinear&quot;      
##  [61] &quot;gaussprPoly&quot;         &quot;gaussprRadial&quot;       &quot;gbm_h2o&quot;            
##  [64] &quot;gbm&quot;                 &quot;gcvEarth&quot;            &quot;GFS.FR.MOGUL&quot;       
##  [67] &quot;GFS.LT.RS&quot;           &quot;GFS.THRIFT&quot;          &quot;glm.nb&quot;             
##  [70] &quot;glm&quot;                 &quot;glmboost&quot;            &quot;glmnet_h2o&quot;         
##  [73] &quot;glmnet&quot;              &quot;glmStepAIC&quot;          &quot;gpls&quot;               
##  [76] &quot;hda&quot;                 &quot;hdda&quot;                &quot;hdrda&quot;              
##  [79] &quot;HYFIS&quot;               &quot;icr&quot;                 &quot;J48&quot;                
##  [82] &quot;JRip&quot;                &quot;kernelpls&quot;           &quot;kknn&quot;               
##  [85] &quot;knn&quot;                 &quot;krlsPoly&quot;            &quot;krlsRadial&quot;         
##  [88] &quot;lars&quot;                &quot;lars2&quot;               &quot;lasso&quot;              
##  [91] &quot;lda&quot;                 &quot;lda2&quot;                &quot;leapBackward&quot;       
##  [94] &quot;leapForward&quot;         &quot;leapSeq&quot;             &quot;Linda&quot;              
##  [97] &quot;lm&quot;                  &quot;lmStepAIC&quot;           &quot;LMT&quot;                
## [100] &quot;loclda&quot;              &quot;logicBag&quot;            &quot;LogitBoost&quot;         
## [103] &quot;logreg&quot;              &quot;lssvmLinear&quot;         &quot;lssvmPoly&quot;          
## [106] &quot;lssvmRadial&quot;         &quot;lvq&quot;                 &quot;M5&quot;                 
## [109] &quot;M5Rules&quot;             &quot;manb&quot;                &quot;mda&quot;                
## [112] &quot;Mlda&quot;                &quot;mlp&quot;                 &quot;mlpKerasDecay&quot;      
## [115] &quot;mlpKerasDecayCost&quot;   &quot;mlpKerasDropout&quot;     &quot;mlpKerasDropoutCost&quot;
## [118] &quot;mlpML&quot;               &quot;mlpSGD&quot;              &quot;mlpWeightDecay&quot;     
## [121] &quot;mlpWeightDecayML&quot;    &quot;monmlp&quot;              &quot;msaenet&quot;            
## [124] &quot;multinom&quot;            &quot;mxnet&quot;               &quot;mxnetAdam&quot;          
## [127] &quot;naive_bayes&quot;         &quot;nb&quot;                  &quot;nbDiscrete&quot;         
## [130] &quot;nbSearch&quot;            &quot;neuralnet&quot;           &quot;nnet&quot;               
## [133] &quot;nnls&quot;                &quot;nodeHarvest&quot;         &quot;null&quot;               
## [136] &quot;OneR&quot;                &quot;ordinalNet&quot;          &quot;ordinalRF&quot;          
## [139] &quot;ORFlog&quot;              &quot;ORFpls&quot;              &quot;ORFridge&quot;           
## [142] &quot;ORFsvm&quot;              &quot;ownn&quot;                &quot;pam&quot;                
## [145] &quot;parRF&quot;               &quot;PART&quot;                &quot;partDSA&quot;            
## [148] &quot;pcaNNet&quot;             &quot;pcr&quot;                 &quot;pda&quot;                
## [151] &quot;pda2&quot;                &quot;penalized&quot;           &quot;PenalizedLDA&quot;       
## [154] &quot;plr&quot;                 &quot;pls&quot;                 &quot;plsRglm&quot;            
## [157] &quot;polr&quot;                &quot;ppr&quot;                 &quot;PRIM&quot;               
## [160] &quot;protoclass&quot;          &quot;qda&quot;                 &quot;QdaCov&quot;             
## [163] &quot;qrf&quot;                 &quot;qrnn&quot;                &quot;randomGLM&quot;          
## [166] &quot;ranger&quot;              &quot;rbf&quot;                 &quot;rbfDDA&quot;             
## [169] &quot;Rborist&quot;             &quot;rda&quot;                 &quot;regLogistic&quot;        
## [172] &quot;relaxo&quot;              &quot;rf&quot;                  &quot;rFerns&quot;             
## [175] &quot;RFlda&quot;               &quot;rfRules&quot;             &quot;ridge&quot;              
## [178] &quot;rlda&quot;                &quot;rlm&quot;                 &quot;rmda&quot;               
## [181] &quot;rocc&quot;                &quot;rotationForest&quot;      &quot;rotationForestCp&quot;   
## [184] &quot;rpart&quot;               &quot;rpart1SE&quot;            &quot;rpart2&quot;             
## [187] &quot;rpartCost&quot;           &quot;rpartScore&quot;          &quot;rqlasso&quot;            
## [190] &quot;rqnc&quot;                &quot;RRF&quot;                 &quot;RRFglobal&quot;          
## [193] &quot;rrlda&quot;               &quot;RSimca&quot;              &quot;rvmLinear&quot;          
## [196] &quot;rvmPoly&quot;             &quot;rvmRadial&quot;           &quot;SBC&quot;                
## [199] &quot;sda&quot;                 &quot;sdwd&quot;                &quot;simpls&quot;             
## [202] &quot;SLAVE&quot;               &quot;slda&quot;                &quot;smda&quot;               
## [205] &quot;snn&quot;                 &quot;sparseLDA&quot;           &quot;spikeslab&quot;          
## [208] &quot;spls&quot;                &quot;stepLDA&quot;             &quot;stepQDA&quot;            
## [211] &quot;superpc&quot;             &quot;svmBoundrangeString&quot; &quot;svmExpoString&quot;      
## [214] &quot;svmLinear&quot;           &quot;svmLinear2&quot;          &quot;svmLinear3&quot;         
## [217] &quot;svmLinearWeights&quot;    &quot;svmLinearWeights2&quot;   &quot;svmPoly&quot;            
## [220] &quot;svmRadial&quot;           &quot;svmRadialCost&quot;       &quot;svmRadialSigma&quot;     
## [223] &quot;svmRadialWeights&quot;    &quot;svmSpectrumString&quot;   &quot;tan&quot;                
## [226] &quot;tanSearch&quot;           &quot;treebag&quot;             &quot;vbmpRadial&quot;         
## [229] &quot;vglmAdjCat&quot;          &quot;vglmContRatio&quot;       &quot;vglmCumulative&quot;     
## [232] &quot;widekernelpls&quot;       &quot;WM&quot;                  &quot;wsrf&quot;               
## [235] &quot;xgbDART&quot;             &quot;xgbLinear&quot;           &quot;xgbTree&quot;            
## [238] &quot;xyf&quot;</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># str(getModelInfo(&quot;knn&quot;, regex = TRUE)) # Por defecto devuelve coincidencias parciales</span>
<span class="kw">modelLookup</span>(<span class="st">&quot;knn&quot;</span>)  <span class="co"># Información sobre hiperparámetros</span></code></pre></div>
<pre><code>##   model parameter      label forReg forClass probModel
## 1   knn         k #Neighbors   TRUE     TRUE      TRUE</code></pre>
<p>Este paquete permite, entre otras cosas:</p>
<ul>
<li><p>Partición de los datos</p>
<ul>
<li><p><code>createDataPartition(y, p = 0.5, list = TRUE, ...)</code>: crea particiones balanceadas de los datos.</p>
<ul>
<li><p>En el caso de que la respuesta <code>y</code> sea categórica realiza el muestreo en cada clase. Para respuestas numéricas emplea cuantiles (definidos por el argumento <code>groups = min(5, length(y))</code>).</p></li>
<li><p><code>p</code>: proporción de datos en la muestra de entrenamiento.</p></li>
<li><p><code>list</code>: lógico; determina si el resultado es una lista con las muestras o un vector (o matriz) de índices</p></li>
</ul></li>
<li><p>Funciones auxiliares: <code>createFolds()</code>, <code>createMultiFolds()</code>, <code>groupKFold()</code>, <code>createResample()</code>, <code>createTimeSlices()</code></p></li>
</ul></li>
<li><p>Análisis descriptivo: <code>featurePlot()</code></p></li>
<li><p>Preprocesado de los datos:</p>
<ul>
<li><p>La función principal es <code>preProcess(x, method = c(&quot;center&quot;, &quot;scale&quot;), ...)</code>, aunque se puede integrar en el entrenamiento (función <code>train()</code>) para estimar los parámetros de las transformaciones a partir de la muestra de entrenamiento y posteriormente aplicarlas automáticamente al hacer nuevas predicciones (p.e. en la muestra de test).</p></li>
<li><p>El parámetro <code>method</code> permite establecer una lista de procesados:</p>
<ul>
<li><p>Imputación: <code>&quot;knnImpute&quot;</code>, <code>&quot;bagImpute&quot;</code> o <code>&quot;medianImpute&quot;</code></p></li>
<li><p>Creación y transformación de variables explicativas: <code>&quot;center&quot;</code>, <code>&quot;scale&quot;</code>, <code>&quot;range&quot;</code>, <code>&quot;BoxCox&quot;</code>, <code>&quot;YeoJohnson&quot;</code>, <code>&quot;expoTrans&quot;</code>, <code>&quot;spatialSign&quot;</code></p>
<p>Funciones auxiliares: <code>dummyVars()</code>…</p></li>
<li><p>Selección de predictores y extracción de componentes: <code>&quot;corr&quot;</code>, <code>&quot;nzv&quot;</code>, <code>&quot;zv&quot;</code>, <code>&quot;conditionalX&quot;</code>, <code>&quot;pca&quot;</code>, <code>&quot;ica&quot;</code></p>
<p>Funciones auxiliares: <code>rfe()</code>…</p></li>
</ul></li>
</ul></li>
<li><p>Entrenamiento y selección de los hiperparámetros del modelo:</p>
<ul>
<li><p>La función principal es <code>train(formula, data, method = &quot;rf&quot;, trControl = trainControl(), tuneGrid = NULL, tuneLength = 3, ...)</code></p>
<ul>
<li><p><code>trControl</code>: permite establecer el método de remuestreo para la evaluación de los hiperparámetros y el método para seleccionar el óptimo, incluyendo las medidas de precisión. Por ejemplo <code>trControl = trainControl(method = &quot;cv&quot;, number = 10, selectionFunction = &quot;oneSE&quot;)</code>.</p>
<p>Los métodos disponibles son: <code>&quot;boot&quot;</code>, <code>&quot;boot632&quot;</code>, <code>&quot;optimism_boot&quot;</code>, <code>&quot;boot_all&quot;</code>, <code>&quot;cv&quot;</code>, <code>&quot;repeatedcv&quot;</code>, <code>&quot;LOOCV&quot;</code>, <code>&quot;LGOCV&quot;</code>, <code>&quot;timeslice&quot;</code>, <code>&quot;adaptive_cv&quot;</code>, <code>&quot;adaptive_boot&quot;</code> o <code>&quot;adaptive_LGOCV&quot;</code></p></li>
<li><p><code>tuneLength</code> y <code>tuneGrid</code>: permite establecer cuantos hiperparámetros serán evaluados (por defecto 3) o una rejilla con las combinaciones de hiperparámetros.</p></li>
<li><p><code>...</code> permite establecer opciones específicas de los métodos.</p></li>
</ul></li>
<li><p>También admite matrices <code>x</code>, <code>y</code> en lugar de fórmulas (o <em>recetas</em>: <code>recipe()</code>).</p></li>
<li><p>Si se imputan datos en el preprocesado será necesario establecer <code>na.action = na.pass</code>.</p></li>
</ul></li>
<li><p>Evaluación de los modelos</p>
<ul>
<li><p><code>postResample(pred, obs, ...)</code>: regresión</p></li>
<li><p><code>confusionMatrix(pred, obs, ...)</code>: clasificación</p>
<ul>
<li>Funciones auxiliares: <code>twoClassSummary()</code>, <code>prSummary()</code>…</li>
</ul></li>
</ul></li>
<li><p>Analisis de la importancia de los predictores:</p>
<ul>
<li><code>varImp()</code>: interfaz a las medidas específicas de los métodos de aprendizaje supervisado (<a href="https://topepo.github.io/caret/variable-importance.html#model-specific-metrics">Sección 15.1</a>) o medidas genéricas (<a href="https://topepo.github.io/caret/variable-importance.html#model-independent-metrics">Sección 15.2</a>).</li>
</ul></li>
</ul>
<p>Ejemplo regresión con KNN:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># caret</span>
<span class="kw">data</span>(Boston, <span class="dt">package =</span> <span class="st">&quot;MASS&quot;</span>)

<span class="kw">library</span>(caret)
<span class="co"># Partición</span>
<span class="kw">set.seed</span>(<span class="dv">1</span>)
itrain &lt;-<span class="st"> </span><span class="kw">createDataPartition</span>(Boston<span class="op">$</span>medv, <span class="dt">p =</span> <span class="fl">0.8</span>, <span class="dt">list =</span> <span class="ot">FALSE</span>)
train &lt;-<span class="st"> </span>Boston[itrain, ]
test &lt;-<span class="st"> </span>Boston[<span class="op">-</span>itrain, ]
<span class="co"># Entrenamiento y selección de hiperparámetros</span>
<span class="kw">set.seed</span>(<span class="dv">1</span>)
knn &lt;-<span class="st"> </span><span class="kw">train</span>(medv <span class="op">~</span><span class="st"> </span>., <span class="dt">data =</span> train,
             <span class="dt">method =</span> <span class="st">&quot;knn&quot;</span>,
             <span class="dt">preProc =</span> <span class="kw">c</span>(<span class="st">&quot;center&quot;</span>, <span class="st">&quot;scale&quot;</span>),
             <span class="dt">tuneGrid =</span> <span class="kw">data.frame</span>(<span class="dt">k =</span> <span class="dv">1</span><span class="op">:</span><span class="dv">10</span>),
             <span class="dt">trControl =</span> <span class="kw">trainControl</span>(<span class="dt">method =</span> <span class="st">&quot;cv&quot;</span>, <span class="dt">number =</span> <span class="dv">10</span>))
<span class="kw">plot</span>(knn)</code></pre></div>
<p><img src="01-introduccion_files/figure-html/unnamed-chunk-24-1.png" width="80%" style="display: block; margin: auto;" /></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">ggplot</span>(knn, <span class="dt">highlight =</span> <span class="ot">TRUE</span>)</code></pre></div>
<p><img src="01-introduccion_files/figure-html/unnamed-chunk-24-2.png" width="80%" style="display: block; margin: auto;" /></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">knn<span class="op">$</span>bestTune</code></pre></div>
<pre><code>##   k
## 3 3</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">knn<span class="op">$</span>finalModel</code></pre></div>
<pre><code>## 3-nearest neighbor regression model</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Interpretación</span>
<span class="kw">varImp</span>(knn)</code></pre></div>
<pre><code>## loess r-squared variable importance
## 
##         Overall
## lstat    100.00
## rm        88.26
## indus     36.29
## ptratio   33.27
## tax       30.58
## crim      28.33
## nox       23.44
## black     21.29
## age       20.47
## rad       17.16
## zn        15.11
## dis       14.35
## chas       0.00</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Evaluación</span>
<span class="kw">postResample</span>(<span class="kw">predict</span>(knn, <span class="dt">newdata =</span> test), test<span class="op">$</span>medv)</code></pre></div>
<pre><code>##     RMSE Rsquared      MAE 
## 4.960971 0.733945 2.724242</code></pre>
<p>Un comentario final:</p>
<blockquote>
<p>“While I’m still supporting caret, the majority of my development effort has gone into the tidyverse modeling packages (called tidymodels)”.</p>
<p>— Max Kuhn, autor del paquete <code>caret</code> (actualmente ingeniero de software en RStudio).</p>
</blockquote>
<p>Kuhn, M. y Wickham, H. (2020). <em>Tidymodels: a collection of packages for modeling and machine learning using tidyverse principles</em>. Version 0.1.1 (2020-07-14). <a href="https://www.tidymodels.org" class="uri">https://www.tidymodels.org</a>.</p>

</div>
<!-- </div> -->
            </section>

          </div>
        </div>
      </div>
<a href="análisis-e-interpretación-de-los-modelos.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="trees.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": true,
"facebook": false,
"twitter": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/rubenfcasal/aprendizaje_estadistico/edit/master/01-introduccion.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["aprendizaje_estadistico.pdf"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
